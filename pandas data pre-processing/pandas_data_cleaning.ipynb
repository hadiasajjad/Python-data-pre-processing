{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "id": "DUHtSgHXz12u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5675ea19-a9d0-42fe-d86a-66964894a668"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 85
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt')\n",
        "import nltk\n",
        "nltk.download('omw-1.4')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the data into a Pandas DataFrame\n",
        "df = pd.read_csv('titanic.csv')\n"
      ],
      "metadata": {
        "id": "QUdb2v6F0bxZ"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert all text in the \"name\" column to lowercase\n",
        "#df['Name'] = df['Name'].str.lower()"
      ],
      "metadata": {
        "id": "ngcd2yVe0juo"
      },
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Remove punctuation from the \"name\" column\n",
        "df['Name'] = df['Name'].apply(lambda x: re.sub('[^\\w\\s]', '', x))\n"
      ],
      "metadata": {
        "id": "jUWHnvY10n9P"
      },
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove stop words from the \"Name\" column\n",
        "stop_words = stopwords.words('english')\n",
        "df['Name'] = df['Name'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n"
      ],
      "metadata": {
        "id": "fLFuN6rd0tlN"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the \"Name\" column\n",
        "#df['Name'] = df['Name'].apply(lambda x: word_tokenize(x))"
      ],
      "metadata": {
        "id": "8lEwio2v0zDd"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lemmatize the \"name\" column\n",
        "#lemmatizer = WordNetLemmatizer()\n",
        "#df['Name'] = df['Name'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
      ],
      "metadata": {
        "id": "SdeBv8AX030p"
      },
      "execution_count": 91,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df['Name'] = df['Name'].astype(str)\n",
        "# df['Name'] = df['Name'].apply(lambda x: x.split())\n",
        "# lemmatizer = WordNetLemmatizer()\n",
        "# df['Name'] = df['Name'].apply(lambda x: [lemmatizer.lemmatize(word) for word in x])"
      ],
      "metadata": {
        "id": "pj9mzCPktOU6"
      },
      "execution_count": 92,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the \"Name\" column back to a string\n",
        "#df['Name'] = df['Name'].apply(lambda x: ' '.join(x))\n",
        "df['Name'] = df['Name'].str.replace(\".\", \"\").apply(lambda x: x.split(\" \"))\n",
        "df['Name'] = df['Name'].apply(lambda x: x[0] + \", \" + \" \".join(x[1:])).str.title()"
      ],
      "metadata": {
        "id": "g_RtAMLf07bA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac1125a0-1808-4399-a1cf-daa43733046f"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-93-2d5223e5249f>:3: FutureWarning: The default value of regex will change from True to False in a future version. In addition, single character regular expressions will *not* be treated as literal strings when regex=True.\n",
            "  df['Name'] = df['Name'].str.replace(\".\", \"\").apply(lambda x: x.split(\" \"))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert \"Sex\" column to binary values (0 for male, 1 for female)\n",
        "#df['Sex'] = df['Sex'].apply(lambda x: 0 if x == 'male' else 1)\n",
        "#df['Sex'] = df['Sex'].map({'male': 'M', 'female': 'F'})\n",
        "df['Sex'] = df['Sex'].fillna('unknown').map({'male': 'M', 'female': 'F', 'unknown': 'U'})"
      ],
      "metadata": {
        "id": "zZn6J6vi1B4A"
      },
      "execution_count": 94,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Replace missing values in \"age\" column with the median age\n",
        "age_median = df['Age'].median()\n",
        "df['Age'].fillna(age_median, inplace=True)"
      ],
      "metadata": {
        "id": "fM5v1FDR1FsN"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values in \"Cabin\" column with \"Unknown\"\n",
        "df['Cabin'].fillna('Unknown', inplace=True)"
      ],
      "metadata": {
        "id": "LoamDD8w1Jr7"
      },
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert \"Embarked\" column to binary values (0 for \"S\", 1 for \"C\", 2 for \"Q\")\n",
        "df['Embarked'] = df['Embarked'].apply(lambda x: 0 if x == 'S' else (1 if x == 'C' else 2))"
      ],
      "metadata": {
        "id": "6MvZBMPJ1Of7"
      },
      "execution_count": 97,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace missing values in \"Fare\" column with the median fare\n",
        "fare_median = df['Fare'].median()\n",
        "df['Fare'].fillna(fare_median, inplace=True)"
      ],
      "metadata": {
        "id": "zybPehgC1Ryl"
      },
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Drop irrelevant columns\n",
        "df.drop(['PassengerId', 'Pclass', 'SibSp', 'Parch', 'Ticket'], axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "XmwOD2iY1VAa"
      },
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the cleaned DataFrame to a CSV file\n",
        "df.to_csv('titaniccleaned_data.csv', index=False)"
      ],
      "metadata": {
        "id": "x_xIB3ADpHPQ"
      },
      "execution_count": 100,
      "outputs": []
    }
  ]
}